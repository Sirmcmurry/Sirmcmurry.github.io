---
title: Research
---

I have broad research interests related to Artificial Intelligence, Phonetics and Statistics. Here I will list a few research papers I have written as a graduate student.

<br>

### Barbie Generator

![](images/Barbie.jpeg){width="562.5"} 

Using GPT 2, a Large Language Model (LLM), I have attempted to Fine-Tune a machine to generate lyrics similar to those written by rapper Nicki Minaj. I will present the generated lyrics produced by GPT 2, lyrics generated by my fine-tuned GPT2 and lyrics generated by Chat GPT to a group of participants who will vote on which lyrics they prefer. While this project does not have direct implications in the world of language study, it can be used a lens to understand the type of stylistic structure these models are capable of learning. In fact, through the processes that I will describe below, a basic understanding of computational engineering is learned. This learning includes, what fine-tuning is, the level computation required to fine-tune, and the differences in the two types and sizes of language models. I have also come to understand the significance of the size of a dataset when it comes to fine tuning. It is also just a very fun project that can easily be done in Google Colab.

Fine tuning has been done to better effectively classify texts and sentiment analysis (Howard & Ruder 2018), and image classification (Church et al. 2021). Fine tuning is essentially transferring the knowledge that a pre-trained language model has learned and applying it in a different task (Church et al. 2021). Put even more simply, by feeding a language model the explicit lyrics of an artist, in principal, that model can use the skills it learned to predict texts to predict text in a specific structure.

The first step in fine-tuning is to choose a language model. I used GPT 2 (Generative Pre-trained Transformer 2, Rubel Schneider et al. 2021) model downloaded directly from Hugging Face. It has millions of parameters and is able to predict the next word in a string (Rubel Schneider et al. 2021). While it is much smaller than Chat GPT (Abdullah et al. 2022) it still has strong predictive power thanks to the underlying networks of neurons and transformers that make up its structure. I attempted to use a second language model, Llama 2 from Meta. It has been trained on Trillions of tokens with billions more parameters than GPT 2 (Jayaseelan, 2023). It too, predicts the next word in a string. However, I ran into numerous issues running it in Colab and in the interest of time I decided best to leave it alone. In addition to importing the proper transformers, I imported the GPT2 tokenizers.

The second step in fine-tuning is to have txt document of the data you want the machine to replicate. In my case, I was interested in producing lyrics in the same style as Nicki Minaj and as such I needed every public song she had written -- except for her newest album. To do this, I asked Microsoft's Copilot to write a Python code that could scrape several urls at once and store what it scraped into a txt document. In a nutshell, web scraping is using software to read through an HTML file and copying the text on that file (Khder, 2021). Using very simple code, I scraped lyrics from a lyrics website and stored them in a txt file. I purposefully only scraped lyrics that were written by Nicki and I did not scrape anything from her new album. 80% of the txt will be set aside as a training set and 20% as a dev set. Typically, between 5% and 10% of the data should be set aside as a test set. However, because I am testing the machine on new lyrics (Minaj's new album) I decided it was more efficacious to use all of the available data in the fine-tuning.

The third step in fine-tuning is training. LLMs like the one I am using in this project have already been trained on trillions of tokens and have a billion or more parameters. In principal, they should be more than capable of producing lyrics by predicting which words should come next. However, by fine-tuning (transferring knowledge of text prediction from chat bot to lyric generating) I expect the models to be able to predict Nicki Minaj type lyrics. After installing the proper transformers and importing the tokenizers, the machines ran through the training set (2 epochs for the GPT 2) and evaluated its performance with the dev set. After that, I was able to use the "generate" command and use new lyrics as prompts. The default setting of the machine's temperature, the amount of randomness it can produce, was set to 0.7. My newly fine-tuned machine has been name Barbie Generator.

Upon first generating a string of text from Barbie Generator, I noticed that it clearly understood the structure and style of Nicki Minaj. However, the text had no meaning. I say that to mean the machine produced text in the same pattern as the txt file but did not produce it in ways that connected to the larger world or even the prewritten lyrics that were the prompts.

In order to validate the results of my fine-tuned model, I compared its output to the output of a GPT 2 model that was not fine-tuned (available on Hugging-Face) and the free version of Chat GPT. All three machines were given the same prompts. However, because the GPT 2 model through Hugging Face had a limited character count, I needed to generate lyrics three or more times, each time removing a little bit of the prompt. I presented five prompts each with three generated lyrics from the three machines to five participants and asked them to vote on their favorites. None of the participants knew which machine generated which.

![](images/Screenshot%202024-10-15%20171757.png)

Unsurprisingly, the participants overwhelmingly preferred the lyrics generated by Chat GPT. It is a much larger language model with better predictive power. I personally believe that it has a stronger understanding of what lyrics are and how lyrics are typically structured. When I included "in the style of Nicki Minaj" in the prompt, it produced text that referenced the lyrics in the prompt. The GPT 2 model which was not fine-tuned actually received more votes than the Barbie Generator. This was completely surprising. The nonfine-tuned model produced text that was utterly random and had no structure to it. Sadly, my creation came in last place with the fewest number of votes. The machine was able to create lyrics in similar structures and styles of Ms. Minaj. For example, in her style of rap there is a great deal of repetition. Usually this is a stylistic choice and done for emphasis but the Barbie Generator clearly did not understand it. It repeated lines multiple times that had no bearing or relation to what came before it. It would also repeat swear words that didn't fully fit in with the rest of the lyrics. If anything it sounded more like the musings of an insane person.

I considered increasing the number of epochs in the training (it was set to two) but decided against it. The data in the training and dev set only contained about 173,000 characters and an even smaller number of tokens. Increasing the number of epochs would only cause the machine to better generate the structure of lyrics but not the meaning behind the lyrics. I also considered changing the temperature of the machine. Regrettably the lyrics it generated became more nonsensical when temperature increased and more repetitive when the temperature decreased.

I think the main reasons why the Barbie Generator did so poorly is in its size and training. GPT 2 is simply a smaller LLM and not as powerful as Chat GPT. The fine tuning data was also small with a relatively small number of tokens, many of which were in dialectal English. Ms. Minaj frequently raps and speaks in her natively dialect of Trinidadian English and features other rappers who rap in their own nonstandard dialects. I suspect the LLMs of a certain size struggle with nonstandard dialects as much of their training data probably comes from mainstream English.

This was a fun project, however, there are serious ethical considerations of this type of work. First, text scraping is in a moral gray area. There does not currently exist federal legislation prohibiting web scraping but there are laws in place to protect copyrighted material (Krotov et al 2020. Krotov et al. 2023). Some websites contain copyrighted material that is protected under law and other outright prohibit web scraping. I myself had to check several lyrics website to find one that did not prohibit me from web scraping. Ethically a web scraper has to make sure not to reveal confidential information and expose private information (Krotov et al 2023). Fortunately, my project did not put private or personal information at risk and I am not producing value from the data I have scraped.

Text generation, especially of lyrics in a specific person's style, also poses some ethical questions. Several authors have suggested that text generation in general can promote bias (Rane et al 2023, Illia et al 2023) and infringe upon copyright protections (Krotov et al 2020). A specific issue that I don't see getting much attention is that mimicking an artist can degenerate the value of that artist's work. If I were able to generate lyrics that could mimic Ms. Minaj I could potentially sell that in a bid to make money or defame here. In 2023, a TikToker named Ghostwriter released a song that he had written but was sung by AI. That is already an impressive feat. The things that concerned the music industry was that it was a song written and sung in the style of rapper Drake and the Weekend (Roth, 2023). Universal Music immediately moved to have the song taken down from Tik Tok and Spotify, where it received millions of views and streams (Roth, 2023, Shanfeld, 2023). In the future, it will be very possible to have a fully developed AI music industry. As this is a very new trend in generation, the legal and ethical implications of these tools has yet to be decided.

In conclusion, I fine-tuned a LLM to produce lyrics similar to that of Nicki Minaj. While it did not perform well when put against a larger LLM it was still able to produce text that had similar structures to the text of Nicki Minaj. Possible limitations of this project included limited fine-tuning data, nonstandard English data, and the size of the LLM used. The ethical implications of this type of work is still being discussed but with time, ethical and legal provisions will be written to address these implications.
